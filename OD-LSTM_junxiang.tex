\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{multirow}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title ( Outer Dictionary Model: An Efficient Network For Named Entity Recognition)
/Author (Junxiang Ge, Qi Zhang, Haizhou Zhao, AI Research Department, Sogou Inc.)
} %Leave this	
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case. 
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, 
% remove them. 

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
%\title{AAAI Press Formatting Instructions \\for Authors Using \LaTeX{} --- A Guide }
\title{ Outer Dictionary Model: An Efficient Network For Named Entity Recognition }
%Your title must be in mixed case, not sentence case. 
% That means all verbs (including short verbs like be, is, using,and go), 
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
%\author{Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{Primarily Mike Hamilton of the Live Oak Press, LLC, with help from the AAAI Publications Committee}\\ \Large \textbf{AAAI Style Contributions by
%Pater Patel Schneider,} \\ \Large \textbf{Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen}\\ % All authors must be in the same font size and format. Use \Large and \textbf to achieve this result when breaking a line
%\textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\ %If you have multiple authors and multiple affiliations
% use superscripts in text and roman font to identify them. For example, Sunil Issar,\textsuperscript{\rm 2} J. Scott Penberthy\textsuperscript{\rm 3} George Ferguson,\textsuperscript{\rm 4} Hans Guesgen\textsuperscript{\rm 5}. Note that the comma should be placed BEFORE the superscript for optimum readability
%2275 East Bayshore Road, Suite 160\\
%Palo Alto, California 94303\\
%publications20@aaai.org % email address must be in roman text type, not monospace or sans serif
%}
\author{Junxiang Ge, Qi Zhang, Haizhou Zhao\\ 
AI Research Department, Sogou Inc. \\
Beijing, China}
 \begin{document}

\maketitle

\begin{abstract}
%AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions. 
With the risen use of AI application, we need to understand some specific type of message from user's query. Named Entity Recognition (NER) is a basic task for that use. Dictionary information is very useful when doing NER, howerver, most of the current method use it in word embedding format, which will bring out the situation that we need to retrain model when the list of words in dictionary is greatly changed. In this paper, we propose a new conception, called Outer Dictionary (OD) model, which means to use the outer type info of gazzete for each word, without using the specific word or word embedding vector. In other word, we try to combine information of knowledge graph into Neural Network (NN). To implement the OD model, we propose a new structure, called Tag Embedding LSTM (TE-LSTM). Experiment results shows that our model acheive a great improvement on tasks within which gazzete is heavily depended on, with comparison with current state-of-the-art methods. 
\end{abstract}

%\noindent Congratulations on having a paper selected for inclusion in an AAAI Press proceedings or technical report! This document details the requirements necessary to get your accepted paper published using PDF\LaTeX{}. If you are using Microsoft Word, instructions are provided in a different document. AAAI Press does not support any other formatting software. 
%
%The instructions herein are provided as a general guide for experienced \LaTeX{} users. If you do not know how to use \LaTeX{}, please obtain assistance locally. AAAI cannot provide you with support and the accompanying style files are \textbf{not} guaranteed to work. If the results you obtain are not in accordance with the specifications you received, you must correct your source file to achieve the correct result. 
%
%These instructions are generic. Consequently, they do not include specific dates, page charges, and so forth. Please consult your specific written conference instructions for details regarding your submission. Please review the entire document for specific instructions that might apply to your particular situation. All authors must comply with the following:
%
%\begin{itemize}
%\item You must use the 2020 AAAI Press \LaTeX{} style file and the aaai.bst bibliography style file, which are located in the 2020 AAAI Author Kit (aaai20.sty and aaai.bst).
%\item You must complete, sign, and return by the deadline the AAAI copyright form (unless directed by AAAI Press to use the AAAI Distribution License instead).
%\item You must read and format your paper source and PDF according to the formatting instructions for authors.
%\item You must submit your electronic files and abstract using our electronic submission form \textbf{on time.}
%\item You must pay any required page or formatting charges to AAAI Press so that they are received by the deadline.
%\item You must check your paper before submitting it, ensuring that it compiles without error, and complies with the guidelines found in the AAAI Author Kit.
%\end{itemize}

\section{Introduction}
Named Entity Recognition (NER) is a base task when doing many Natural Language Processing (NLP) tasks. Usually, we need to extract mainly information from users' sentences to help us understand their meanings, and then use the extracted information to do further process. For example, in weather domain, we may need to extract \textbf{date and position} to help us understand when and where the user is asked towards the weather; in music type service, we may need to track \textbf{singer, song, style } entities to suit user's command; for many commom use, we need \textbf{person, location, organization} message to understand a sentence.

To accomplish this task, many method has been explored.  Convolution Neural Network (CNN) is used in Image Classification (Krizhevsky, Sutskever and Geoffrey 2012), and then proved also powerful in Text Classification (Kim 2014). CNN is also useful in NER tasks, whether to capture character-level features (Chiu and Nichols 2016; Ma and Hovy 2016; Peters et al. 2017), or to do word-level sequence labelling (Santos, Xiang, and Zhou 2015; Strubell et al. 2017). To learn the correlation between words in sentence, Recurrent Neural Network (RNN) (Sutskever, Vinyals and Le 2014) and Long Short-Term Memory (LSTM)  (Sak, Senior and Beaufays 2014) are proposed, and they achieve great success on machine translation area and so on. They are then used in NER tasks and achieve significant improvement (Chiu and Nichols 2016; Ma and Hovy 2016; Peters et al. 2017). These structures can be use in different situations (Yang, Liang, and Zhang 2018). As the depth of Nerual Network (NN) usually improve the final performance, more and more complicated method has been proposed in recent years. Transformer structure (Vaswani et al. 2017) and its derivative methods, like OpenAI GPT (Radford et al. 2018), BERT (Devlin et al. 2018), XLNet (Yang et al. 2019), make innovative attempt to NN structures. 

Among these years, many method focus on base layer of NN structure, usually the embedding input layer, and then directly use it to do NLP tasks (Peters et al. 2018; Radford et al. 2018; Devlin et al. 2018; Yang et al. 2019). Though will it make improvement, it usually pays a long time to train the model. But in these way, it tell us that embedding input can directly infect the result of NLP tasks. 

There exists a little difference between English NER tasks and Chinese NER tasks. In Chinsese or other hieroglyphics language, character means a single word; and word is a character phrase which has length longer than one. In English or other letter base language, character means a single letter, and word is a composition of letters. English word is composed by characters, for which character level CNN can help improve NER tasks (Huang et al. 2015; Chiu and Nichols 2016; Ma and Hovy 2016; Peters et al. 2017; Yang, Liang, and Zhang 2018); but that can not be use in Chinese NER tasks, which is hieroglyphics. For this reason, Chinese NER tasks is usually done by word baseline or character baseline structure. Word baseline structure refer to using word segmentor to split sentence into word tokens and then use their word vector (Peng and Dredez 2015; Peng and Dredez 2016); while character structure simply treat each sentence into combination of single Chinese character, and then using character embedding to do NLP tasks (Zhang and Yang 2018; Gui et al. 2019). In these methods, it seems useful when using word embedding within character baseline structure, or the oppisite. 

Most of state-of-the-art NER methods uses Conditional Random Fields (CRF) (Lafferty, McCallum, and Pereira 2011) to decode the sequence labelling result. CRF can work even without any NN cells. Hand-writing trait can sometimes already achieve high performance (Finkel, Grenager, and Manning 2005; Okazaki 2007). 

In some situations, word backgound knowledge, namely dictionary (the same meaning as lexicon and gazette), is neccessary to do NER jobs. For example, in music service case, “You Raise Me Up'' would be treated more likely as a chat information rather than a song, unless the model is given that knowledge (see in Figure \ref{fig1}). Most of the state-of-the-art methods using word embedding to complete these task. But it then gives two problems: 

\begin{itemize}
\item Out Of Vacabulary (OOV) problem. As for \textbf{song} entity in music service, word embedding size will be too large; and sometimes will the length of song be too long to make word vector. 
\item Retrain problem. When the content of dictionary changes abundantly, the trained model would have poor performance doing the same jobs. Still use the \textbf{song} entity as an example, the dictionary would be changed monthly or even daily, thus we need to retrain our model synchronously. 
\end{itemize}

Faced with this problem, we proposed a new conception, called Outer Dictionary (OD) models, which means using the dictionary's outer trait, without using its specific content. For example, we use the word type `` song ``, rather than a specific song name, to help model understand the outer information. In these way, word embedding is no longer needed for Chinese NER task. Exprement result show that in gazzete depended-on task, OD model gives the best result among state-of-the-art method. And when the content of dictionary changed, the model can synchronously update its result without retraining.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{gazzete_is_needed} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
\caption{In certain NER situation, dictionary info is neccessary, as cases showed in figure. With knowing the certain name is a song, we tend to denote the word as a song rather a common sentence.}
\label{fig1}
\end{figure}

\subsection{Contributions}

In summary, we make these contributions:

\begin{itemize}
\item We proposed a new conception: Outer Dictionary (OD) model, which is used to update the content of dictionary without retraining the model.
\item In order to realize OD model, we explored Tag Embedding LSTM (TE-LSTM), using the word type, rather the word itself, to improve the performance of NER tasks.
\item Experiment result shows our model gives significant improvements on gazzete depended-on tasks among state-of-the-art models.
\item We release the music service dataset, which we use in our paper, for further scientific research.
\end{itemize}

\section{Related Work}
Success of BERT (Devlin et al. 2018) and XLNet (Yang et al. 2019) show that word embedding is important and useful for many upper NLP tasks. More useful information is there in the vector, more efficiently the model would work. To a great degree, embedding layer decides the performance. Same conclusion can be made from a lot of NER models, as for character level message, word level message, position message, handwriting message and so on. Especially in some specific NER task, like music service, gazzete is heavilly depended on to make better performance. 

Most of the recent models use gazette as word embedding input, or its string format, to give the model related message. Lattice LSTM (Zhang and Yang 2018) use the embedding as additional input to character baseline LSTM, which can give the model more sentence information. Lexivon Rethinking CNN (Gui et al. 2019), corparate the word embedding vector to correlated CNN layer, and the coflict word segment could be learned by rethinking. Stanford NER (Finkel, Grenager, and Manning 2005) can use the gazette information, as string format, in CRF model.

However, in some pratical use, the content of the gazette will change greatly and constantly. In this way, most of the recent model need to be retrained after every abundantly change of gazette. To solve this problem, we need to construct a model which can still be used after such change. We then came out with the OD models, and use TE-LSTM to realize these model. As a result, with no more word embedding is needed, and model needed only to be trained once.  What we need is the word tag information to help the model better understand user's intention.

\section{Outer Dictionary LSTM}

In this chapter, we illustrate the model we use to realize the OD model. Our framework is based on Tensorflow (Abadi et al. 2015). We use the character baseline model for Chinese NER task,  inspired by the results of Lattice LSTM (Zhang and Yang 2018) and Lexicon Rething CNN (Gui et al. 2019). In order to better understand the model, we make a appointment that: In Chinsese or other hieroglyphics language, character means a single word; and word is a character phrase which has length longer than one. In English or other letter base language, character means a single letter, and a word is a composition of letters; then a phrase is composited of a set of words. For Chinese NER jobs, only character embedding will we use, and no more word embedding is needed, for we hold the following opinions towards these jobs:

\begin{itemize}
\item Common characters in hieroglyphics language, can be exhaustive,  especially for Chinese, because there are no more than 100, 000 characters in simplified Chinese. 
\item Words in hieroglyphics language, will be uncountable, because there are still many new words or pharases are still created every day. Thus, OOV error can not be avoided in these case.
\end{itemize}

Thus, in Chinese NER jobs, we denote a sentence as a composition of many characters: $s = (c_1, c_2, ..., c_n) $ as the base input to model.

\subsection{Character-Base LSTM}

We use character embedding lookup table to turn character id to vector:

\begin{equation}
a_i = e_c(c_i) \label{char_embedding}
\end{equation}

Usually a dropout layer is connected with the embedding output, which is proved useful in NER jobs (Ma and Hovy 2016). After that, bi-directional LSTM (Bi-LSTM), the most common structure used in NER tasks, is applied to learn the character connection within the sentence. We use the hidden vector for LSTM output, that is, for input: $x = (a_1, a_2, ..., a_n) $,  we get output: $h = (h_1, h_2, ..., h_n)$, where $h_i$ is the concatenation of forward ($\overrightarrow{h_i}$) and backward ($\overleftarrow{h_i}$) LSTM hidden vector:

\begin{equation}
h_i = [\overrightarrow{h_i};\overleftarrow{h_i}] \label{lstm_out}
\end{equation}

\subsection{Tag Embedding Model}

As case show in Figure \ref{fig1}, we need to use dictionary for certain NER tasks. However, the constant variation of dictionary may cause constant retrain problem. Therefore, we may not use the specific word in each dictionary, instead, only the outer information will be used, which is called Outer Dictionary (OD) model. Here, we implement our OD model using Tag Embedding (TE), which uses the tag embedding of the dictionary. Cause we use LSTM structure to utilize tag embedding info, we called our model  TE-LSTM.

Specifically, we do this by two step: 

a)  Match sentence with the word in dictionary using certain outer model method.

b)  Turn match result into embedding information, used as the input of LSTM.

\textbf{Match Policy}. In this part, we need to match all the word in given dictionary occured in current sentence. We impletement this by using Aho-Corasick Automation Algorithm (Aho and Corasick 1975). In summary, we complete a KMP (Knuth, Morris, and Pratt 1977) + Trie Tree (Briandais 1959) algorithm to accomplish this task. In methematical, given sentence $s = (c_1, c_2, ..., c_n) $, and a dictionary $d_i$, which is composed of word set $D_i=\{w_1, w_2, ..., w_V\}$. We denote the result of match policy as:

\begin{equation}
r_{d_i} = [ (f_1, b_1), (f_2 , b_2), ..., (f_k, b_k) ] \label{match_result}
\end{equation}

Here, $f_i$ and $b_i$ denote the front and back position of each match word. Let $W_{f_i,b_i}$ represent the substring of sentence $s$ with the rank $[f_i, b_i]$, then each $(f_i, b_i)$ tuple forms a word in $D_i$. Parameter $k$ means the total number of such word matched in $d_i$, which is uncertain.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{tag_scheme} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
\caption{Illustration for tag embedding scheme. For dictionary `song', there are two possible songs, the longer one is put in the first place (column 1). For dictionary `language', only one possible result will be matched, however in this case, since we set the tag blank number to 2, we should add one more padding blank to fill the position, which is showed as column 4. }
\label{fig2}
\end{figure}

\textbf{Tag Embedding Scheme}. After match policy is fullfilled, we need to turn that result into vector so that it can be understood by model. Cause the result of match policy is denoted as position, we can simply use it to get a new sequence form from original sentence. With dictionary $d_i$ and a matched word position rank range $[f_i, b_i]$, the original sentence can be written as: $ s = (c_1, c_2, ..., c_{f_i - 1}, c_{f_i}, ..., c_{b_i}, c_{b_i + 1}, ..., c_n) $, we then apply tag scheme for these sequence: turn the match word to correspond tag as shown in Figure \ref{fig2}, the unmatche part is turn to tag ` O `.

Here, we choose \textbf{IOBES} scheme for it is proved better than IOB1 and IOB2 tag scheme in result by previous works (Chiu and Nichols 2016; Ma and Hovy 2016). Yet we need to emphasis that: the dictionary here need not to be the same with the result tag species. As an illustration, to extract \textbf{name} entity, we can use dictionary \textbf{first name} and dictionary \textbf{last name} to do this transformation.

Since there will be one or more matched word for one dictionary, in order to fix the matched word of $d_i$ in certain position, we choose $w$ words from result list. Let us refer $w$ as \textbf{tag blank number}. We choose the best $w$ results from the matched list (in our experiment, here we use the word length, the longer, the prior). Then for dictionary $d_i$, if the number of match results isn't less than $w$, then the best $w$ results will be choose and turn to a new sequence; or the full results will be included, and some padding blank will added to that dictionary's leave position to fill the blank, until the number of sequence of that dictionary is fullfilled.  The full process of producing tag sequence is shown in Figure \ref{fig2}. We do the tag scheme in this way cause we think the position is important for model to understand, Of course, $w$ could be variable for different dictionary, but that needs a bit more efforts and seems low performance increase.

With the tag embedding sequence produced, denoting each tag sequence as: $s_t = (t_1, t_2, ..., t_n)$, we turn the result to vector using: 

\begin{equation}
g_i = e_t(t_i) \label{tag_embedding}
\end{equation}

where $e_t$ is the tag embedding lookup table. We use random generated number to produce this lookup table. Then the char embedding output $a_i$ and tag embeddding output $g_i$ is concatenated together:
\begin{equation}
x_i = (a_i;g_i) \label{concat}
\end{equation}
The concat input is used as additional property to enrich the input information (Collobert et al. 2011), and then fed into the Bi-LSTM layer.  

\subsection{Loss and Training}

CRF layer is applied on the top, same with many previous works and proved to be useful (Collobert et al. 2011; Chiu and Nichols 2016; Ma and Hovy 2016; Yang, Liang, and Zhang 2018; Zhang and Yang 2018; Gui et al. 2019). Denote $y_j$ as a possible label sequence for sentence $s_i$ and $Y$ as the total set of all possible $y_j$, then the probability of $y_j$ is:

\begin{equation}
p(y_j | s_i) = softmax(\sum_i{W_{c_i} * h_i} + B_{c_i})_j
\label{crf_probability}
\end{equation}

$W_{c_i}$, $B_{c_i}$ is respectively the emission matrix and transformation matrix correlate to $h_i$, and $j$ is the rank for $y_j$ in all possible label sequence set $Y$.  While training, we take the $L_2$ regularization for all parameters into account together with CRF loss. With batch number $N$ and corresponding gold set $(s_i, y_i)$, the final loss can be computed as:

\begin{equation}
L = \sum_{i=1}^{N}{log(p(y_i | s_i))} + \frac{\lambda}{2}{|| \theta ||}^2
\label{crf_probability}
\end{equation}

$\lambda$ is the $L_2$ hyper parameter, and $\theta$ stands for all the training parameters. At decode step, we use the Viterbi algorithm (Viterbi 1967) to decode result, finding the label sequence with the highest score: 

\begin{equation}
y^* = \mathop{\arg\max}_{y_i \in Y}{p(y_i | s_i)}
\label{crf_decode}
\end{equation}

\section{Experiments}

\subsection{Settings}

Here displays the experiment settings within our work.

\textbf{Dataset}. We use the music domain dataset obtained on China Conference on Knowledge Graph and Semantic Computing (CCKS) 2018 music dataset. Cause we can only get the training dataset,  we split that file into three part with portion 8:1:1 as train set, dev set and test set. Call this dataset as \textbf{CCKS} dataset. 

\begin{table}[t]
\caption{Statistics of Sogou MusicNER dataset.}\smallskip
\centering
\resizebox{.8\columnwidth}{!}{
\smallskip\begin{tabular}{l|c|c|c}
\hline
Count & Sentences & Tokens & Entities\\
\hline
Train & 7,177 & 55,613 & 8,585 \\
Dev & 897 & 6,858 & 1,057 \\
Test & 897 & 6,775 & 1,047 \\
\hline
\end{tabular}
}
\label{table1}
\end{table}

Another music domain dataset is annotated by ourselves, and the tags defined is richer than CCKS. The statistics information of this dataset is listed in Table \ref{table1}. Call it \textbf{Sogou MusicNER} dataset.

\textbf{Character Embedding}. In order to compare with Lattice LSTM (Zhang and Yang 2018) and Lexicon Rethinking CNN (Gui et al. 2019), we use the word vector provided in their papers (these two paper use the same character embedding and word embedding). There are character embedding and word embedding, we use the latter one as character embedding for we find it perform better in both character-level LSTM and OD LSTM.

\textbf{Tag Embedding}. Given tag embedding length $l_t$ as hyper parameter, we use uniform random policy to generate the tag embedding lookup table. 

\textbf{Word Embedding}. For Lattice LSTM and Lexicon Rethinking CNN, they use the same word embedding file, which can not incorparate the word of dictionary directly. For Stanford NER (Finkel, Grenager, and Manning 2005) model and OD model, we use the dictionary file, which in true is a list of word name. For the same dataset, the dictionary they used keep the same. We choose the dictionary according to the follow principles:

\begin{itemize}
\item For exhaustive tag, we list all the items of that tag as a dictionary. For example, entity \textbf{Country} and \textbf{Project}.
\item For limited but updated constantly tag, we list the current item of that tag as a dictionary. For example, entity \textbf{Singer} and \textbf{Song}.
\item For tag that is unlimited but has common prefix or suffix, we list all the common prefix or suffix for it as a dictionary. For example, entity \textbf{Organization} usually has a common suffix, such as: inc., company, league.
\item For unlimited and no rule tag, we don't use dictionary for it.
\item We build the content of tag mainly through knowledge graph we collected, some of the dictionary are enriched with the dataset.
\end{itemize}

\subsection{Compared Models}



\begin{table}[t]
\caption{Hyperparameters}\smallskip
\centering
\resizebox{.95\columnwidth}{!}{
\smallskip\begin{tabular}{l|c|c}
\hline
Layer & Hyper-parameters & Value \\
\hline
Char-layer & embeddding size & 50 \\
\hline
Tag-layer & embeddding size & 10 \\
& tag blank number & 3 \\
\hline
LSTM & state size & 200 \\
 & number of layers & 1 \\
\hline
Dropout & dropout rate & [0.0, 0.5] \\
\hline
 & batch size & [1, 10] \\
 & initial learning rate & 0.015 \\
 & decay rate & 0.05 \\
 & momentum & 0.9 \\
 & gradient clipping & 5.0 \\
 & L2 $\lambda$ & 1e-8 \\
 & fine tune & True \\
\hline
\end{tabular}
}
\label{table2}
\end{table}

For compared purposed, we impletement these models below:

\textbf{Lattice LSTM}. The author of Lattice LSTM (Zhang and Yang 2018) has generously provided his code on website. We direcly use his code and settings, and run on experiment dataset. We denote it as La-LSTM.

\textbf{Lexicon Rethinking CNN}. Same with Lattice LSTM, we use the code provided by the author (Gui et al. 2019) and run on our dataset. We denote it as LR-CNN.

\textbf{Character LSTM}. We basicly impletement the character-level LSTM as the first model, no any other properties (like bichar, softword, position etc.) used. We use the word embedding in Lattice LSTM as character embedding here for it gives better performance. We abbreviate it as C-LSTM.

\textbf{Stanford NER model}. Stanford NER tool (Finkel, Grenager, and Manning 2005) is used in our work to do comparision, for it is convenient to add handcrafted features into model, and use dictionary (called gazette in its term) directly, which can be compared to our OD model intuitively. For short, denote it as S-NER. 

\textbf{Tag Embedding LSTM}. This is the model which we proposed in this paper. The character embedding is the same with the word embedding provided by the author of Lattice LSTM (Zhang and Yang 2018). The dictionary used is the same with S-NER model for comparision. We denote it as TE-LSTM.

\subsection{Hyper-parameters}

Shown in Table \ref{table2}, we choose the hyper-parameters referring to BLSTM-CNNs-CRF structure (Ma and Hovy 2016). 

\begin{itemize}
\item \textbf{Fine-tuning}.We apply fine-tuning on all the embedding vector, which is proved useful by previous works (Collobert et al. 2011; Peng and Dredze 2015; Ma and Hovy 2016). 
\item \textbf{Dropout}. We choose the dropout rate between 0.0 and 0.5, for different dataset, the best result will occur at different rate.
\item \textbf{SGD policy}. We use stochastic gradient descent (SGD) policy to train model, with momentum set as 0.9.
\end{itemize}

\subsection{Main Results}

\begin{table}[t]
\caption{Main Results}\smallskip
\centering
\resizebox{.95\columnwidth}{!}{
\smallskip\begin{tabular}{l|ccc|ccc}
\hline
\multirow{2}*{Model} & \multicolumn{3}{|c|}{CCKS} & \multicolumn{3}{|c}{MusicNER} \\
\cline{2-7}
~ & Precision & Recall & F1 & Precision & Recall & F1 \\
\hline
La-LSTM  & 87.34\% & 75.03\% &  80.72\% & 86.24\% & 84.43\% & 85.33\% \\
LR-CNN  & 88.17\% & 76.98\% & 82.2\% & 85.99\% & 85.00\% & 85.49\% \\
\hline
C-LSTM & 75.66\% & 71.12\% & 73.32\% & 84.11\% & 82.50\% & 83.30\% \\
ST-NER & \textbf{91.00\%} & 76.15\% & 82.92\% & 88.00\% & 85.48\% & 86.72\% \\
TE-LSTM & 84.28\% & \textbf{84.51\%} & \textbf{84.40\%} & \textbf{93.09\%} & \textbf{92.82\%} & \textbf{92.96\%} \\
\hline
\end{tabular}
}
\label{table3}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{change_after_adding_song} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
\caption{After adding words into dictionary, the new entities can be recognized by the model.}
\label{fig3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{change_after_delete_song} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
\caption{After delete existing words in dictionary, the corresponding entities can be ignored by the model.}
\label{fig4}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{no_change_after_delete_song} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
\caption{After delete existing words in dictionary, some of the the result will not change.}
\label{fig5}
\end{figure}

Compared to previsous work, we report the best result of our experiment here.

\textbf{Music Domain Result}. Table \ref{table3} shows the result of music domain dataset. Results show that our model report the best result on both dataset for F1 score. With dictionary information given, we can see that ST-NER shows better performance than La-LSTM and LR-CNN. That, to some extent, cause by the lack of music domain word information. Compared to ST-NER, our model gives \textbf{1.48\%} percent promotion of F1 score on \textbf{CCKS} dataset, and \textbf{6.24\%} percent promotion of F1 score on \textbf{MusicNER} dataset. This result proves that TE-LSTM is useful to utilize the information of dictionary, though without specially making word embedding vector for each word in lexicon.

\textbf{Without Retraining}. We show it here that TE-LSTM need not to retrain the model when dictionary is greatly changed. The cost of retraining a model turns to the cost of maintaining the dictionaries, which is more convenient and can be human understood and modified. 

Here, we use \textbf{song} entities as example. Figure \ref{fig3} shows the difference after adding corresponding words as song, the model can recognize this entities with the new given knowledge. Figure \ref {fig4} shows the change after deleting redundant words in dictionary, the model will ignore these words. But there are also cases which will not change or partly change after deleting the words, shown in Figure \ref{fig5},  these cases exist because there exist certain sentence phrase to discriminate the word as a song,  or part of the word is still in the dictionary.



\section{Conclusion}

We proposed a new model to better explored the information of dictionary without make embedding for each word, which helps us need not to retrain a model after the content of dictionary is greatly changed. And the result shows that our model reach higher performance on dictionary-depended task, which certificated that our model is a powerful structure to utilize the information of dictionary. And with OD model, the cost of training a new model is replaced by protecting the dictionaries, which is more understandable to human beings.

We have several dicrections to move ahead. For the structure itself, OD model can also be simply implemented using the Lattice LSTM structure, which also, is a powerful way to use lexicon message. Furthermore, the information of dictionary can be reached on website or knowledge graph, which can be more convinient and promising. On this way, the model need to understand and consult the needed information by itself. Our model is just the start of combining outer knowledge into neural network model. 

\section{ Acknowledgments }
Especially grateful to Can Cui (Chengdu, AI Research Department, Sogou Inc.) for cooperation on the Sogou MusicNER dataset, to Jindou Wu (Hangzhou, AI Research Department, Sogou Inc.) for advice on data processing of CCKS 2018 music dataset.

\section{References}

\smallskip \noindent
Krizhevsky, A.; Sutskever, I.; and Geoffrey E. H. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Proceedings of  Advances in Neural Information Processing Systems 25 (NIPS 2012).

\smallskip \noindent
Kim, Y. 2014. Convolutional Neural Networks for Sentence Classification. arXiv:1408.5882.

\smallskip \noindent
Sutskever, I.; Vinyals, O.; and Le, Q.V. 2014. Sequence to Sequence Learning with Neural Networks. In Proceedings of Advances in Neural Information Processing Systems 27 (NIPS 2014).

\smallskip \noindent
Sak, H.; Senior, A.; and Beaufays, F. 2014. Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition. arXiv:1402.1128.

\smallskip \noindent 
Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12 (2011) 2493-2537. 

\smallskip \noindent
Santos. C. N.; Xiang, B.; and Zhou, B. 2015. Classifying Relations by Ranking with Convolutional Neural Networks. arXiv:1504.06580. 

\smallskip \noindent
Strubell, E.; Verga, P.; Belanger, D.; and McCallum, A. 2017. Fast and Accurate Entity Recognition with Iterated Dilated Convolutions. arXiv:1702.02098. 

\smallskip \noindent
Peters, M. E.; Ammar, W.; Bhagavatula, C.; and Power. R.  2017. Semi-supervised sequence tagging with bidirectional language models. arXiv:1705.00108.

\smallskip \noindent 
Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. arXiv:1802.05365. 

\smallskip \noindent
Ma, X.; and Hovy, E. 2016. End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. arXiv:1603.01354. 

\smallskip \noindent
Yang, J.; Liang, S.; and Zhang, Y. 2018. Design Challenges and Misconceptions in Neural Sequence Labeling. arXiv:1806.04470. 

\smallskip \noindent
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention Is All You Need. arXiv:1706.03762. 

\smallskip \noindent
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018. Improving Language Understanding by Generative Pre-Training. Technical report, Open AI.

\smallskip \noindent
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805.

\smallskip \noindent
Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; and Le, Q. V. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv:1906.08237.

\smallskip \noindent
Sang, E. F.; and Meulder, F. D. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. arXiv:cs/0306050. 

\smallskip \noindent
Chiu, J. P.C.; and Nichols, E. 2016. Named Entity Recognition with Bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics, Volume 4, 2016, p.357-370. 

\smallskip \noindent
Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv:1508.01991. 

\smallskip \noindent
Peng, N.; and Dredze, M. 2015. Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). 

\smallskip \noindent
Peng, N.; and Dredze, M. 2016. Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning. arXiv:1603.00786. 

\smallskip \noindent
He, H.; and Sun, X. 2016. F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media. 	arXiv:1611.04234. 

\smallskip \noindent
Chen, Q.; Zhuo, Z.; and Wang, W. 2019. BERT for Joint Intent Classification and Slot Filling. arXiv:1902.10909. 

\smallskip \noindent
Zhang, Y.; and Yang, J. 2018. Chinese NER Using Lattice LSTM. arXiv:1805.02023. 

\smallskip \noindent
Gui, T.; Ma, R.; Zhang, Q.; Zhao, L.; Jiang, Y.; and Huang, Y. 2019. CNN-Based Chinese NER with Lexicon Rethinking. In Proceedings of the International Joint Conference on Artificial Intelligence 2019 (IJCAI 2019). 

\smallskip \noindent
Lafferty, J.; McCallum, A.; and Pereira, F. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the 18th International Conference on Machine Learning 2001 (ICML 2001), pages 282-289.

\smallskip \noindent
Socher, R.; Manning, C. D.; and Andrew Y. Ng. 2010. Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop. 

\smallskip \noindent
Finkel, J. R.; Grenager, T.; and Manning, C. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. 

\smallskip \noindent
Okazaki, N. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs). URL http://www. chokkan. org/software/crfsuite. 

\smallskip \noindent
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR. 

\smallskip \noindent
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and Dean, J. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS. 

\smallskip \noindent
Mikolov, T.; Yih, W.; and Zweig, G. 2013. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT. 

\smallskip \noindent
Pennington, J.; Socher, R.; and Manning, C.D. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543. 

\smallskip \noindent
Mikolov, T.; Grave, E.; Bojanowski, P.; Puhrsch, C.; and Joulin, A. 2017. Advances in Pre-Training Distributed Word Representations. arXiv:1712.09405. 

\smallskip \noindent
Aho, A. V.; and Corasick, M. J. 1975. Efficient string matching: An aid to bibliographic search. Communications of the ACM. June 1975, 18 (6): 333–340.

\smallskip \noindent
Knuth, D.; Morris, J. H.; and Pratt, V. 1977. Fast pattern matching in strings. SIAM Journal on Computing. 6 (2): 323–350. 

\smallskip \noindent
Briandais, D. L. R. 1959. File searching using variable length keys. Proc. Western J. Computer Conf. pp. 295–298. 

\smallskip \noindent
Viterbi AJ. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory. 13 (2): 260–269. doi:10.1109/TIT.1967.1054010. 

\smallskip \noindent
Liu, K.; Guo, S.; Liu, S.; Zhang, Y. 2018. China Conference on Knowledge Graph and Semantic Computing, 2018. https://biendata.com/competition/CCKS2018\_2/data/. 

\smallskip \noindent
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems. https://www.tensorflow.org.



\end{document}
